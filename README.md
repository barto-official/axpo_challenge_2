# IoT Data Collection, Aggregation, Access Layer using Microsoft Azure

This project aims to create a data engineering infrastructure for IoT domain to build a system that allows for an easy access of data to Business Intelligence (BI) specialists and Data Scientists. The solution is implemented using Microsoft Azure and involves:

* generating (fake, for simuluation) IoT data — implemented as Docker container inside Azure Web App
* aggregating data for analysis — in the same Docker container inside Azure Web App
* collecting and storing data in raw format — in MySQL on Azure
* API service to provide access to IoT data collected and stored in a MySQL database — implemented as a separate Docker container inside Azure Web App


Components
1. Data Sources:
   * Sensors: Defined in the `sensors.json` file, each sensor has attributes such as latitude, longitude, unit, type, range, and description.
   * The sensors simulate temperature data from different locations within an infrastructure (e.g., Main Entrance, Kitchen, Deposit, Assembly Floor, Offices).
2. Data Generation:
   * `generator.py`: The main script that generates data from the defined sensors. It initializes the sensors and asynchronously gathers data from them. This script uses the Sensor class (from sensor.py) to create sensor objects and simulate data readings within specified ranges.
3. Configuration and Settings:
   * `settings.py`: Contains configuration settings for the application, including Azure Event Hub connection details and general settings like logging levels and intervals for data generation.
   * `sensors.json`: Specifies the configuration of each sensor, including its location, measurement unit, and range.
4. Event Hub Client:
   * EventHubProducerClient: Utilized within `generator.py` to establish a connection to Azure Event Hub using credentials and settings defined in settings.py.

5. Data Transmission:
   * azure-eventhub: The library used to send data to Azure Event Hub. Data generated by sensors is sent as events to Azure Event Hub for real-time analytics and storage.


## How the system works:
1. Data is generated inside Docker container deployed as Web App to Microsoft Azure.
    * Frequency of generation is 1 second.
    * There are data from 5 sensors
2. Data is aggregated (1 minute interval) in the same container.
    * Both types of data are send in Subscriber-Producer mode into Azure Eventhub
    * Data is sent in the JSON format

3. Azure Function picks data from Azure Eventhub and inserts them into MySQL deployed in Azure as well
4. Each Azure Function is responsible for each JSON so 1 second frequency activation
5. Data Access Layer is built in the form of FastAPI as a separate Docker Container, deployed to Microsoft Azure

<img width="331" alt="Screenshot 2024-06-14 at 12 47 20" src="https://github.com/barto-official/axpo_challenge_2/assets/125658269/d2860ab3-2e4f-487c-909e-e395077346e3">


**Why this architecture and technologies? There are three main reasons:**
- Minimization of Costs as it was just a proof of concept.
- Time Constraints
- Effectiveness of the System despite the constraints. 

**What should be done differently at this stage:**
- Use MQTT protocol instead of sending data in JSON format
- Use Microsoft Azure Eventgrid instead of Azure Eventhub to ingest data 
- Create Pipeline using Azure Data Factory rather than Azure Functions if possible


**Ideal Architecture**


![Untitled Diagram1 drawio](https://github.com/barto-official/axpo_challenge_2/assets/125658269/d0a68cff-4ec4-4564-8734-fc8f074aeede)



